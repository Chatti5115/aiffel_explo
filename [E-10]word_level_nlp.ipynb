{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:06.707966Z",
     "start_time": "2020-09-24T02:42:06.704999Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import re    \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.039493Z",
     "start_time": "2020-09-24T02:42:06.708861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수: 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146630</th>\n",
       "      <td>This can't be what it looks like.</td>\n",
       "      <td>Ça ne peut pas être ce que ça paraît.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70716</th>\n",
       "      <td>I just got back in town.</td>\n",
       "      <td>Je viens de revenir en ville.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145702</th>\n",
       "      <td>Please don't make me go with Tom.</td>\n",
       "      <td>S'il te plaît, ne m'oblige pas à y aller avec ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40855</th>\n",
       "      <td>It's a little dated.</td>\n",
       "      <td>C'est un peu désuet.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24483</th>\n",
       "      <td>Aren't you sleepy?</td>\n",
       "      <td>N'as-tu pas sommeil ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      eng  \\\n",
       "146630  This can't be what it looks like.   \n",
       "70716            I just got back in town.   \n",
       "145702  Please don't make me go with Tom.   \n",
       "40855                It's a little dated.   \n",
       "24483                  Aren't you sleepy?   \n",
       "\n",
       "                                                      fra  \\\n",
       "146630              Ça ne peut pas être ce que ça paraît.   \n",
       "70716                       Je viens de revenir en ville.   \n",
       "145702  S'il te plaît, ne m'oblige pas à y aller avec ...   \n",
       "40855                                C'est un peu désuet.   \n",
       "24483                               N'as-tu pas sommeil ?   \n",
       "\n",
       "                                                       cc  \n",
       "146630  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "70716   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "145702  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "40855   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "24483   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수:', len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.057125Z",
     "start_time": "2020-09-24T02:42:07.040354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89192</th>\n",
       "      <td>I'm disappointed with you.</td>\n",
       "      <td>Tu me déçois.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69825</th>\n",
       "      <td>His house is by a river.</td>\n",
       "      <td>Sa maison est au bord de la rivière.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69576</th>\n",
       "      <td>He is quite a gentleman.</td>\n",
       "      <td>C'est un gentleman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68971</th>\n",
       "      <td>Do you like this garden?</td>\n",
       "      <td>Vous aimez ce jardin ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86345</th>\n",
       "      <td>He lived there by himself.</td>\n",
       "      <td>Il vivait là seul.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              eng                                   fra\n",
       "89192  I'm disappointed with you.                         Tu me déçois.\n",
       "69825    His house is by a river.  Sa maison est au bord de la rivière.\n",
       "69576    He is quite a gentleman.                   C'est un gentleman.\n",
       "68971    Do you like this garden?                Vous aimez ce jardin ?\n",
       "86345  He lived there by himself.                    Il vivait là seul."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][60000:93000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.065580Z",
     "start_time": "2020-09-24T02:42:07.058132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You were there, right?', 'You will stay at home.',\n",
       "       \"You won't be punished.\", ..., 'What time did you wake up?',\n",
       "       'What time did you wake up?', 'What time did you wake up?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_np_eng= lines['eng'].to_numpy()\n",
    "lines_np_fra= lines['fra'].to_numpy()\n",
    "lines_np_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정제, 정규화, 전처리 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소문자 변경 후 구두점 분리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.078380Z",
     "start_time": "2020-09-24T02:42:07.066612Z"
    }
   },
   "outputs": [],
   "source": [
    "sos_token = '<start> '\n",
    "eos_token = ' <end>'\n",
    "\n",
    "def preprocess_line(line, plus_token = True):\n",
    "    # 소문자로 변경하기\n",
    "    line = line.lower().strip()\n",
    "    # 구두점(Punctuation)을 단어와 분리하기\n",
    "    line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
    "    line = re.sub(r'[\" \"]+', \" \", line)\n",
    "    line = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", line)\n",
    "\n",
    "    line = line.strip()\n",
    "    \n",
    "    if plus_token == True:\n",
    "        line = sos_token + line + eos_token\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 단위로 토큰화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.088838Z",
     "start_time": "2020-09-24T02:42:07.079546Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=7000,  \n",
    "        filters=' ',   \n",
    "        oov_token=\"<unk>\"  \n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)  \n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어, 프랑스어 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.664834Z",
     "start_time": "2020-09-24T02:42:07.089865Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_lines = []\n",
    "fra_lines = []\n",
    "\n",
    "# eng_lines.append(lines.eng.apply(lambda x : preprocess_line(x,plus_token = False)))\n",
    "# fra_lines.append(lines.fra.apply(lambda x : preprocess_line(x),))\n",
    "\n",
    "for eng, fra in zip(lines.eng, lines.fra):\n",
    "    if len(eng) == 0: continue\n",
    "    if len(fra) == 0: continue   \n",
    "        \n",
    "    eng_lines.append(preprocess_line(eng, plus_token = False))\n",
    "    fra_lines.append(preprocess_line(fra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:07.672511Z",
     "start_time": "2020-09-24T02:42:07.665883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(eng_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.316454Z",
     "start_time": "2020-09-24T02:42:07.673413Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 15, 105, 23, 374, 34, 19, 7, 22, 8, 6, 3],\n",
       " [2, 15, 3036, 151, 78, 4, 3],\n",
       " [2, 15, 13, 1135, 8, 3723, 4, 3],\n",
       " [2, 10, 19, 347, 8, 16, 1519, 4, 3],\n",
       " [2, 15, 13, 389, 8, 16, 1519, 4, 3],\n",
       " [2, 10, 19, 1647, 8, 17, 4, 3],\n",
       " [2, 15, 443, 176, 9, 5098, 4, 3],\n",
       " [2, 15, 443, 176, 9, 35, 20, 128, 278, 4, 3],\n",
       " [2, 15, 443, 176, 9, 26, 841, 72, 4, 3],\n",
       " [2, 10, 1136, 176, 9, 10, 841, 72, 4, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tensor, eng_tokenizer = tokenize(eng_lines)\n",
    "fra_tensor, fra_tokenizer = tokenize(fra_lines)\n",
    "fra_tensor[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input, target 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.408003Z",
     "start_time": "2020-09-24T02:42:08.320511Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input = eng_tensor\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[char for char in line if char != fra_tokenizer.word_index['<end>']] for line in fra_tensor]\n",
    "# 시작 토큰 제거\n",
    "decoder_target =[[char for char in line if char != fra_tokenizer.word_index['<start>']] for line in fra_tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.411266Z",
     "start_time": "2020-09-24T02:42:08.409121Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_tensor(tensor):\n",
    "    total_data_text = list(tensor)\n",
    "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "    max_tokens = max(num_tokens)\n",
    "#     max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    maxlen = int(max_tokens)\n",
    "    tensor = pad_sequences(tensor, padding='post', maxlen=maxlen)  \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.724842Z",
     "start_time": "2020-09-24T02:42:08.412137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 10)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_tensor(encoder_input)\n",
    "decoder_input = pad_tensor(decoder_input)\n",
    "decoder_target = pad_tensor(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.728291Z",
     "start_time": "2020-09-24T02:42:08.725786Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index)+1\n",
    "\n",
    "max_eng_seq_len = encoder_input.shape[1]\n",
    "max_fra_seq_len = decoder_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.750134Z",
     "start_time": "2020-09-24T02:42:08.729282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 5795\n",
      "프랑스어 단어장의 크기 : 8297\n",
      "영어 시퀀스의 최대 길이 10\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, test dataset 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.777325Z",
     "start_time": "2020-09-24T02:42:08.751082Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:42:08.784171Z",
     "start_time": "2020-09-24T02:42:08.778467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 10)\n",
      "(30000, 17)\n",
      "(30000, 17)\n",
      "(3000, 10)\n",
      "(3000, 17)\n",
      "(3000, 17)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:49:26.091256Z",
     "start_time": "2020-09-24T02:49:25.476357Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "encoder_inputs = Input(shape=(None, ), name='encoder_input')\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size,\n",
    "                    input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, dropout = 0.5, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:49:26.784028Z",
     "start_time": "2020-09-24T02:49:26.193095Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, dropout = 0.5, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:49:26.914694Z",
     "start_time": "2020-09-24T02:49:26.898013Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:49:27.032908Z",
     "start_time": "2020-09-24T02:49:27.023537Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T02:49:27.773281Z",
     "start_time": "2020-09-24T02:49:27.768596Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    2967040     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 512)    4248064     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 512)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 512)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 2099200     masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  2099200     masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8297)   4256361     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 15,669,865\n",
      "Trainable params: 15,669,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:12:07.759334Z",
     "start_time": "2020-09-24T02:49:31.333993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 54s 21ms/step - loss: 1.9346 - val_loss: 1.5865\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 1.4684 - val_loss: 1.3613\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 1.2802 - val_loss: 1.2470\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 1.1495 - val_loss: 1.1510\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 1.0476 - val_loss: 1.0886\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.9663 - val_loss: 1.0442\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.9005 - val_loss: 1.0155\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.8490 - val_loss: 1.0045\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.8114 - val_loss: 0.9942\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.7759 - val_loss: 0.9825\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.7447 - val_loss: 0.9736\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.7233 - val_loss: 0.9769\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.7125 - val_loss: 0.9850\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.7003 - val_loss: 0.9804\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6868 - val_loss: 0.9823\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6660 - val_loss: 0.9679\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6407 - val_loss: 0.9618\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6284 - val_loss: 0.9670\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6210 - val_loss: 0.9702\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6155 - val_loss: 0.9738\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.6066 - val_loss: 0.9737\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5962 - val_loss: 0.9706\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5865 - val_loss: 0.9659\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5800 - val_loss: 0.9656\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5728 - val_loss: 0.9637\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5665 - val_loss: 0.9653\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5616 - val_loss: 0.9644\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5501 - val_loss: 0.9579\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.5375 - val_loss: 0.9541\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5336 - val_loss: 0.9603\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5296 - val_loss: 0.9580\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5282 - val_loss: 0.9581\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5001 - val_loss: 0.9580\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.4958 - val_loss: 0.9572\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4921 - val_loss: 0.9533\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4873 - val_loss: 0.9563\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4840 - val_loss: 0.9544\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4809 - val_loss: 0.9526\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4779 - val_loss: 0.9526\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.4752 - val_loss: 0.9559\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.4717 - val_loss: 0.9538\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4682 - val_loss: 0.9544\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.4655 - val_loss: 0.9547\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 0.4620 - val_loss: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c8ebc1f40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], \n",
    "          y=decoder_target_train, \n",
    "          validation_data = ([encoder_input_test, decoder_input_test], \n",
    "                             decoder_target_test),\n",
    "          batch_size=32, \n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:12:15.046533Z",
     "start_time": "2020-09-24T03:12:15.041653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 512)         2967040   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 512), (None, 512) 2099200   \n",
      "=================================================================\n",
      "Total params: 5,066,240\n",
      "Trainable params: 5,066,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:33.679780Z",
     "start_time": "2020-09-24T03:14:33.525419Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(embedding_size,))\n",
    "decoder_state_input_c = Input(shape=(embedding_size,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:34.154054Z",
     "start_time": "2020-09-24T03:14:34.152297Z"
    }
   },
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:34.938203Z",
     "start_time": "2020-09-24T03:14:34.931647Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 512)    4248064     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  2099200     embedding_2[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8297)   4256361     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 10,603,625\n",
      "Trainable params: 10,603,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:35.496509Z",
     "start_time": "2020-09-24T03:14:35.492924Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <start>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:36.008471Z",
     "start_time": "2020-09-24T03:14:36.006508Z"
    }
   },
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:36.350914Z",
     "start_time": "2020-09-24T03:14:36.348567Z"
    }
   },
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<start>']) and i!=fra2idx['<end>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T03:14:38.844455Z",
     "start_time": "2020-09-24T03:14:37.314057Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: the food looks delicious . \n",
      "정답 문장: les plats ont l air d licieux . \n",
      "번역기가 번역한 문장:  le sais le le le \n",
      "-----------------------------------\n",
      "입력 문장: i m allergic to gluten . \n",
      "정답 문장: je suis allergique au gluten . \n",
      "번역기가 번역한 문장:  je suis . de les au\n",
      "-----------------------------------\n",
      "입력 문장: that s just what he needs . \n",
      "정답 문장: c est pr cis ment ce qu il lui faut . \n",
      "번역기가 번역한 문장:  c n pr ce ce just\n",
      "-----------------------------------\n",
      "입력 문장: i hope you re happy , too . \n",
      "정답 문장: j esp re que vous tes galement heureuse . \n",
      "번역기가 번역한 문장:  j esp entendu vou\n",
      "-----------------------------------\n",
      "입력 문장: three of these are tom s . \n",
      "정답 문장: il y en a trois tom . \n",
      "번역기가 번역한 문장:  il y y ce d d . \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,201,501,1004,2015]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마무리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 문장: i want to ask you something .   \n",
    "정답 문장: je veux vous demander quelque chose .   \n",
    "번역기가 번역한 문장:  je veux te vous quelqu  \n",
    "\n",
    "입력 문장: stop playing hard to get .   \n",
    "정답 문장: cessez de faire ceux qui ne sont pas int ress s !  <br>\n",
    "번역기가 번역한 문장:  arr de de faire fair  \n",
    "\n",
    "입력 문장: you never have any money .   \n",
    "정답 문장: tu ne disposes jamais d aucun argent .   \n",
    "번역기가 번역한 문장:  tu n jamais d d d d   \n",
    "\n",
    "입력 문장: tom slipped and nearly fell .   \n",
    "정답 문장: tom <unk> et <unk> tomber .   \n",
    "번역기가 번역한 문장:  tom a les et en en   \n",
    "\n",
    "입력 문장: tom kept the window closed .   \n",
    "정답 문장: tom garda la fen tre ferm e .   \n",
    "번역기가 번역한 문장:  tom tom la la la la l  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역기가 번역한 문장이 완벽하지는 않지만, 어느정도 비슷한 결과를 보이고 있습니다.  \n",
    "학습 데이터가 더 많거나, 전처리 부분에 신경을 많이 쓴다면 더 좋은 결과를 얻을 것이라고 생각 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
